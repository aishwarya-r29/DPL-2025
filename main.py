# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fI-yDcEH8YzbMl6Lu2bG1XdADiWJUkle
"""

from google.colab import drive
drive.mount('/content/drive')

import os

# Path to your dataset folder
data_path = "/content/drive/MyDrive/Colab Notebooks/TrendDataset"

# List all files in the dataset folder
files = os.listdir(data_path)
print("Files in dataset folder:", files)

import pandas as pd

dataframes = {}
for file in files:
    file_path = os.path.join(data_path, file)
    df_name = file.replace(".csv", "")
    try:
        # Try utf-8 first
        dataframes[df_name] = pd.read_csv(file_path, encoding="utf-8")
    except UnicodeDecodeError:
        # Fallback to latin1 if utf-8 fails
        dataframes[df_name] = pd.read_csv(file_path, encoding="latin1")

# Show dataset names and shapes
for name, df in dataframes.items():
    print(f"{name}: {df.shape}")

# Show column names for each dataset
for name, df in dataframes.items():
    print(f"\n{name} â†’ {df.shape}")
    print(df.columns.tolist()[:10])  # show only first 10 cols for brevity

# Step 5: Inspect schema + sample rows for each loaded DataFrame
import re
pd.set_option("display.max_columns", 100)

def detect_keys(cols):
    cols_lower = [c.lower() for c in cols]
    country_candidates = [c for c in cols if re.search(r'country|area|reporter|reporterdesc|area|iso', c, re.I)]
    iso_candidates = [c for c in cols if re.search(r'iso|iso3|reporteriso|partneriso', c, re.I)]
    year_candidates = [c for c in cols if re.search(r'\byear\b|refyear|yr|refperiod|period|yearcode', c, re.I)]
    return country_candidates, iso_candidates, year_candidates

for name, df in dataframes.items():
    print("\n" + "="*80)
    print(f"Dataset: {name}    shape: {df.shape}")
    print("- columns (showing all):")
    print(list(df.columns))
    country_cols, iso_cols, year_cols = detect_keys(df.columns)
    print("\nDetected linking key candidates:")
    print("  Country-like columns:", country_cols if country_cols else "None detected")
    print("  ISO-like columns:    ", iso_cols if iso_cols else "None detected")
    print("  Year-like columns:   ", year_cols if year_cols else "None detected")
    print("\nSample rows (first 3):")
    display(df.head(3))
    # show value counts for detected keys if present (up to 10)
    if country_cols:
        c = country_cols[0]
        print(f"\n  Sample unique values for '{c}':", pd.Series(df[c].dropna().unique()[:10]).tolist())
    if iso_cols:
        i = iso_cols[0]
        print(f"  Sample unique values for '{i}':", pd.Series(df[i].dropna().unique()[:10]).tolist())
    if year_cols:
        y = year_cols[0]
        print(f"  Sample unique values for '{y}':", pd.Series(df[y].dropna().unique()[:10]).tolist())

# Step 2: Dataset overview
for name, df in dataframes.items():
    print(f"\n===== {name} =====")
    print("Shape:", df.shape)
    print("Columns:", df.columns.tolist())
    print("Data types:\n", df.dtypes.head())
    print("Missing values:\n", df.isnull().sum().head())
    print("\nSample rows:")
    print(df.head(3))
    print("-" * 80)

import seaborn as sns
import matplotlib.pyplot as plt

# Step 3: Missing value visualization
for name, df in dataframes.items():
    plt.figure(figsize=(10, 4))
    sns.heatmap(df.isnull(), cbar=False, cmap="viridis")
    plt.title(f"Missing Values Heatmap - {name}", fontsize=14)
    plt.show()

# Step 4: Cleaning missing values
cleaned_dataframes = {}

for name, df in dataframes.items():
    df_clean = df.copy()

    # Drop columns with more than 40% missing values
    threshold = len(df_clean) * 0.4
    df_clean = df_clean.dropna(axis=1, thresh=threshold)

    # Fill numeric missing values with median
    for col in df_clean.select_dtypes(include=['float64', 'int64']).columns:
        df_clean[col] = df_clean[col].fillna(df_clean[col].median())

    # Fill categorical missing values with mode
    for col in df_clean.select_dtypes(include=['object']).columns:
        if df_clean[col].isnull().sum() > 0:
            df_clean[col] = df_clean[col].fillna(df_clean[col].mode()[0])

    cleaned_dataframes[name] = df_clean
    print(f"{name}: Cleaned shape {df_clean.shape}, Missing values left: {df_clean.isnull().sum().sum()}")

# Step 5: Schema Alignment - Standardize column names
standardized_dataframes = {}

def clean_column_names(columns):
    return [c.strip().lower().replace(" ", "_").replace("-", "_") for c in columns]

for name, df in cleaned_dataframes.items():
    df_std = df.copy()
    df_std.columns = clean_column_names(df_std.columns)

    # Rename common columns if present
    rename_map = {
        "country_name": "country",
        "nation": "country",
        "iso_code": "country",
        "year_": "year",
        "time": "year",
        "value_": "value"
    }
    df_std.rename(columns={c: rename_map.get(c, c) for c in df_std.columns}, inplace=True)

    standardized_dataframes[name] = df_std
    print(f"{name}: Columns -> {df_std.columns.tolist()}")

# Step 6: Exploratory Data Analysis (EDA)

for name, df in standardized_dataframes.items():
    print("="*80)
    print(f"ðŸ“Š Dataset: {name}")
    print("-"*80)
    print("Shape:", df.shape)
    print("\nColumns & Types:")
    print(df.dtypes)
    print("\nMissing Values:")
    print(df.isnull().sum())
    print("\nSummary Statistics (numeric columns):")
    print(df.describe(include='all').transpose().head(10))  # limit output
    print("\nSample Rows:")
    print(df.head(3))
    print("="*80, "\n")

# Step 7: Finding common keys across datasets

from collections import Counter

all_columns = []

for name, df in standardized_dataframes.items():
    all_columns.extend(df.columns.tolist())

# Count frequency of each column name across datasets
col_counter = Counter(all_columns)

print("ðŸ“Œ Column Frequency Across Datasets:")
for col, freq in col_counter.most_common():
    print(f"{col} -> {freq} datasets")

# Show potential keys
potential_keys = [col for col, freq in col_counter.items() if freq >= 2]
print("\nâœ… Potential common keys for merging:", potential_keys)

# Step 8: Data Cleaning & Standardization

import numpy as np

cleaned_dataframes = {}

for name, df in standardized_dataframes.items():
    df = df.copy()

    # Standardize column names again (safety)
    df.columns = df.columns.str.strip().str.lower()

    # Handle missing values
    df.replace(["N/A", "NA", "-", "--", ""], np.nan, inplace=True)

    # Standardize Year column if exists
    if "year" in df.columns:
        df["year"] = pd.to_numeric(df["year"], errors="coerce")

    # Standardize Country column if exists
    if "country" in df.columns:
        df["country"] = df["country"].str.strip().str.title()  # e.g., "united states"

    # Drop duplicates
    df.drop_duplicates(inplace=True)

    cleaned_dataframes[name] = df

print("âœ… Data cleaning & standardization completed.")

# Step 9: Exploratory Data Analysis (EDA)

import matplotlib.pyplot as plt

for name, df in cleaned_dataframes.items():
    print(f"\nðŸ“Š Dataset: {name}")
    print("Shape:", df.shape)
    print("Missing values per column:\n", df.isnull().sum())
    print("\nDescriptive Statistics:\n", df.describe(include="all"))

    # If numeric columns exist, plot distributions
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

    for col in numeric_cols[:3]:  # limit to first 3 numeric columns per dataset
        plt.figure(figsize=(6, 4))
        df[col].hist(bins=30, edgecolor='black')
        plt.title(f"{name} - Distribution of {col}")
        plt.xlabel(col)
        plt.ylabel("Frequency")
        plt.show()

    # If year and one numeric col exist, plot trend
    if "year" in df.columns and len(numeric_cols) > 0:
        plt.figure(figsize=(8, 4))
        df.groupby("year")[numeric_cols[0]].mean().plot()
        plt.title(f"{name} - Trend of {numeric_cols[0]} over Years")
        plt.xlabel("Year")
        plt.ylabel(numeric_cols[0])
        plt.show()

# Step 10: Correlation & Relationship Analysis

import seaborn as sns

for name, df in cleaned_dataframes.items():
    print(f"\nðŸ”— Correlation Analysis for: {name}")

    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

    if len(numeric_cols) > 1:
        corr = df[numeric_cols].corr()
        print(corr)

        # Heatmap
        plt.figure(figsize=(8, 6))
        sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
        plt.title(f"Correlation Heatmap - {name}")
        plt.show()

    # Example: Compare GDP or trade volume by country if available
    if "country" in df.columns and len(numeric_cols) > 0:
        plt.figure(figsize=(10, 5))
        df.groupby("country")[numeric_cols[0]].mean().sort_values().plot(kind="bar")
        plt.title(f"Average {numeric_cols[0]} by Country - {name}")
        plt.ylabel(numeric_cols[0])
        plt.show()

# Step 11: Feature Engineering

engineered_dataframes = {}

for name, df in cleaned_dataframes.items():
    df_fe = df.copy()

    # Ensure lowercase column names for easier handling
    cols = [c.lower() for c in df_fe.columns]
    df_fe.columns = cols

    # Feature 1: Trade Balance
    if "exports" in cols and "imports" in cols:
        df_fe["trade_balance"] = df_fe["exports"] - df_fe["imports"]
    else:
        print(f"Skipping 'trade_balance' for {name}: 'exports' or 'imports' not found.")

    # Feature 2: Trade Openness
    if "gdp" in cols and "exports" in cols and "imports" in cols:
        df_fe["trade_openness"] = (df_fe["exports"] + df_fe["imports"]) / df_fe["gdp"]
    else:
        print(f"Skipping 'trade_openness' for {name}: 'gdp', 'exports', or 'imports' not found.")

    # Feature 3: Export-to-GDP Ratio
    if "gdp" in cols and "exports" in cols:
        df_fe["export_gdp_ratio"] = df_fe["exports"] / df_fe["gdp"]
    else:
        print(f"Skipping 'export_gdp_ratio' for {name}: 'gdp' or 'exports' not found.")

    # Feature 4: Import-to-GDP Ratio
    if "gdp" in cols and "imports" in cols:
        df_fe["import_gdp_ratio"] = df_fe["imports"] / df_fe["gdp"]
    else:
        print(f"Skipping 'import_gdp_ratio' for {name}: 'gdp' or 'imports' not found.")

    # Feature 5: Year-on-Year Growth Rates (if 'year' column exists)
    if "year" in cols:
        df_fe = df_fe.sort_values("year")
        for col in ["gdp", "exports", "imports"]:
            if col in df_fe.columns:
                df_fe[f"{col}_growth"] = df_fe[col].pct_change()
            else:
                print(f"Skipping '{col}_growth' for {name}: '{col}' not found.")

    engineered_dataframes[name] = df_fe
    print(f"\nâœ¨ Engineered features added for {name}: {df_fe.columns.tolist()}")
    display(df_fe.head())

# Employment & Unemployment trends
df_emp = dataframes["Employment_Unemployment"]

# Ensure 'Year' column exists (or similar)
print("Columns in Employment_Unemployment:", df_emp.columns)

# If 'Year' and 'Unemployment_Rate' exist:
if "Year" in df_emp.columns and "Unemployment_Rate" in df_emp.columns:
    plt.figure(figsize=(10,5))
    plt.plot(df_emp["Year"], df_emp["Unemployment_Rate"], marker="o", label="Unemployment Rate (%)")
    plt.xlabel("Year")
    plt.ylabel("Unemployment Rate (%)")
    plt.title("Employment & Unemployment Trends")
    plt.legend()
    plt.grid(True)
    plt.show()

# Show a preview of the dataset
df_emp.head()

# Replace ".." with NaN
df_emp.replace("..", np.nan, inplace=True)

# Melt the year columns
year_cols = df_emp.columns[4:]  # Columns from 2000 onwards
df_emp_melt = df_emp.melt(id_vars=["Country Name", "Country Code", "Series Name", "Series Code"],
                          value_vars=year_cols,
                          var_name="Year",
                          value_name="Value")

# Extract numeric year from 'Year' column
df_emp_melt["Year"] = df_emp_melt["Year"].str.extract("(\d{4})").astype(int)

# Convert Value to float
df_emp_melt["Value"] = df_emp_melt["Value"].astype(float)

# Preview the reshaped dataframe
df_emp_melt.head()

from sklearn.linear_model import LinearRegression

# Filter youth unemployment series (ages 15-24)
df_youth = df_emp_melt[df_emp_melt["Series Name"].str.contains("ages 15-24", na=False)]

# Function to project 2030
def project_2030(df_country):
    df_country = df_country.dropna(subset=["Value"])
    if df_country.empty:
        return np.nan
    X = df_country["Year"].values.reshape(-1,1)
    y = df_country["Value"].values
    model = LinearRegression()
    model.fit(X, y)
    return model.predict([[2030]])[0]

# Apply for each country and series
youth_2030 = df_youth.groupby(["Country Name", "Series Name"]).apply(project_2030).reset_index()
youth_2030.columns = ["Country Name", "Series Name", "Projected_2030"]

# Highlight countries projected >25%
high_youth_unemp = youth_2030[youth_2030["Projected_2030"] > 25]

high_youth_unemp

# Filter only youth unemployment series
youth_unemp_2030 = youth_2030[youth_2030["Series Name"].str.contains("Unemployment")]

# Filter countries with projected youth unemployment > 25%
high_youth_unemp_2030 = youth_unemp_2030[youth_unemp_2030["Projected_2030"] > 25]

# Sort descending by projected unemployment
high_youth_unemp_2030 = high_youth_unemp_2030.sort_values(by="Projected_2030", ascending=False)

# Display
high_youth_unemp_2030.reset_index(drop=True)

import matplotlib.pyplot as plt
import seaborn as sns

# Set figure size
plt.figure(figsize=(12,6))

# Barplot of countries vs projected youth unemployment
sns.barplot(
    data=high_youth_unemp_2030,
    x="Country Name",
    y="Projected_2030",
    palette="Reds_r"
)

# Rotate x labels for readability
plt.xticks(rotation=45, ha='right')

# Labels and title
plt.xlabel("Country")
plt.ylabel("Projected Youth Unemployment (%) in 2030")
plt.title("Countries with Youth Unemployment >25% in 2030")

plt.tight_layout()
plt.show()

import pandas as pd

# Paths to your CSV files
path_2000_2012 = "/content/drive/MyDrive/Colab Notebooks/TrendDataset/2000-2012_Export.csv"
path_2013_2024 = "/content/drive/MyDrive/Colab Notebooks/TrendDataset/2013-2024_Export.csv"

# Load CSVs with ISO-8859-1 encoding
df_export_2000_2012 = pd.read_csv(path_2000_2012, encoding='ISO-8859-1', low_memory=False)
df_export_2013_2024 = pd.read_csv(path_2013_2024, encoding='ISO-8859-1', low_memory=False)

# Quick check
print(df_export_2000_2012.shape, df_export_2013_2024.shape)

# Merge 2000-2012 and 2013-2024 export datasets
df_export_total = pd.concat([df_export_2000_2012, df_export_2013_2024], ignore_index=True)

# Keep only relevant columns
df_export_total = df_export_total[['reporterDesc', 'partnerDesc', 'fobvalue']]

# Drop rows where any of these are missing
df_export_total = df_export_total.dropna(subset=['reporterDesc', 'partnerDesc', 'fobvalue'])

# Convert to string before stripping spaces
df_export_total['reporterDesc'] = df_export_total['reporterDesc'].astype(str).str.strip()
df_export_total['partnerDesc'] = df_export_total['partnerDesc'].astype(str).str.strip()

# Convert fobvalue to numeric, coercing errors
df_export_total['fobvalue'] = pd.to_numeric(df_export_total['fobvalue'], errors='coerce')

# Drop rows where fobvalue could not be converted
df_export_total = df_export_total.dropna(subset=['fobvalue'])

# Reset index
df_export_total = df_export_total.reset_index(drop=True)

print(df_export_total.shape)
df_export_total.head()

# Filter out placeholder values
df_export_filtered = df_export_total[~df_export_total['reporterDesc'].isin(['X', '0'])]
df_export_filtered = df_export_filtered[~df_export_filtered['partnerDesc'].isin(['0', 'X', 'World'])]

# Step 1: Group by reporter and partner to sum fobvalue
df_grouped = df_export_filtered.groupby(['reporterDesc', 'partnerDesc'])['fobvalue'].sum().reset_index()

# Step 2: Compute total exports per reporter
total_exports = df_grouped.groupby('reporterDesc')['fobvalue'].sum().reset_index()
total_exports = total_exports.rename(columns={'fobvalue': 'total_export'})

# Step 3: Merge total exports with grouped data
df_grouped = df_grouped.merge(total_exports, on='reporterDesc')

# Step 4: Compute share of each partner
df_grouped['share'] = df_grouped['fobvalue'] / df_grouped['total_export']

# Step 5: Trade Dependency Index = max share per reporter
tdi_df = df_grouped.groupby('reporterDesc')['share'].max().reset_index()
tdi_df = tdi_df.rename(columns={'share': 'Trade_Dependency_Index'})

# Step 6: Sort by TDI descending to identify most dependent countries
tdi_df = tdi_df.sort_values(by='Trade_Dependency_Index', ascending=False).reset_index(drop=True)

# Show top 10 most dependent countries
tdi_df.head(10)