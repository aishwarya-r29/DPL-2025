{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7JZwxw1244YcV5bKifiiy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishwarya-r29/DPL-2025/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lDagr3a4teoK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba222356-067f-4a54-93e0-fce5954c3612"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# ============================\n",
        "# STEP 1: Load & Merge Data\n",
        "# ============================\n",
        "def load_trend_data(folder_path=\"/content/drive/MyDrive/Colab Notebooks/TrendDataset\"):\n",
        "    all_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
        "    df_list = []\n",
        "\n",
        "    for file in all_files:\n",
        "        try:\n",
        "            # Try utf-8 first\n",
        "            df = pd.read_csv(file, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
        "        except UnicodeDecodeError:\n",
        "            # Fallback to latin1\n",
        "            df = pd.read_csv(file, encoding=\"latin1\", on_bad_lines=\"skip\")\n",
        "\n",
        "        # Standardize year column names (2000 [YR2000] â†’ 2000)\n",
        "        df.rename(columns=lambda x: str(x).split()[0] if \"YR\" in str(x) else x, inplace=True)\n",
        "        df_list.append(df)\n",
        "\n",
        "    # Concatenate all CSVs\n",
        "    merged = pd.concat(df_list, axis=0, ignore_index=True)\n",
        "\n",
        "    # Extract only numeric year columns\n",
        "    year_cols = [c for c in merged.columns if str(c).isdigit()]\n",
        "    merged = merged[year_cols].transpose()  # years as rows\n",
        "\n",
        "    merged.index = merged.index.astype(int)\n",
        "    merged = merged.sort_index()  # ensure chronological order\n",
        "\n",
        "    # Convert '..' to NaN\n",
        "    merged.replace('..', np.nan, inplace=True)\n",
        "\n",
        "    return merged\n",
        "\n",
        "# ============================\n",
        "# STEP 2: Prepare Data\n",
        "# ============================\n",
        "def prepare_sequences(data, look_back=5):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - look_back):\n",
        "        X.append(data[i:i+look_back])\n",
        "        y.append(data[i+look_back])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# ============================\n",
        "# STEP 3: Build LSTM Model\n",
        "# ============================\n",
        "def build_model(input_shape):\n",
        "    model = Sequential([\n",
        "        LSTM(64, activation='tanh', input_shape=input_shape, return_sequences=True),\n",
        "        Dropout(0.2),\n",
        "        LSTM(32, activation='tanh'),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# ============================\n",
        "# MAIN EXECUTION\n",
        "# ============================\n",
        "if __name__ == \"__main__\":\n",
        "    # Load dataset\n",
        "    data = load_trend_data(\"/content/drive/MyDrive/Colab Notebooks/TrendDataset\")\n",
        "    print(\"Data Shape:\", data.shape)\n",
        "\n",
        "    # --- Pick one indicator for univariate forecasting ---\n",
        "    series = None\n",
        "    chosen_col = None\n",
        "    look_back = 5  # Define look_back here\n",
        "\n",
        "    for col in data.columns:\n",
        "        candidate = data[col].dropna()\n",
        "        # Ensure candidate has enough data points to create at least one sequence\n",
        "        if len(candidate) >= look_back + 1:\n",
        "            series = candidate\n",
        "            chosen_col = col # Store the chosen column index\n",
        "            break\n",
        "\n",
        "    if series is None:\n",
        "        print(f\"âŒ No column found with at least {look_back + 1} non-NaN data points after cleaning.\")\n",
        "    else:\n",
        "        print(f\"âœ… Using column with index: {chosen_col} and shape: {series.shape}\")\n",
        "        values = series.values.reshape(-1, 1)\n",
        "\n",
        "        # Normalize\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled = scaler.fit_transform(values)\n",
        "\n",
        "        # Prepare sequences\n",
        "        X, y = prepare_sequences(scaled, look_back=look_back)\n",
        "        X = X.reshape((X.shape[0], X.shape[1], 1))  # [samples, timesteps, features]\n",
        "\n",
        "        print(\"X Shape:\", X.shape, \"y Shape:\", y.shape)\n",
        "\n",
        "        # Build & train\n",
        "        model = build_model((X.shape[1], X.shape[2]))\n",
        "        model.fit(X, y, epochs=20, batch_size=16, verbose=1)\n",
        "\n",
        "        # Save model in the native Keras format\n",
        "        os.makedirs(\"model_files\", exist_ok=True)\n",
        "        model.save(\"model_files/Model.keras\")\n",
        "        print(\"âœ… LSTM Model saved at model_files/Model.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f59d5c-ddd2-47c6-e705-2b97f0b170fb",
        "id": "RyPzy8QWM-ru"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2517528818.py:40: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  merged.replace('..', np.nan, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Shape: (25, 370237)\n",
            "âœ… Using column with index: 41938 and shape: (10,)\n",
            "X Shape: (5, 5, 1) y Shape: (5, 1)\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 0.6709\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.6319\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.5874\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.5454\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.5228\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.4903\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.4640\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.4189\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.3983\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.3625\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.3383\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.3021\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.2863\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.2767\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.2399\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.2180\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.1922\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.1776\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1606\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1506\n",
            "âœ… LSTM Model saved at model_files/Model.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model in the native Keras format\n",
        "model = load_model(\"model_files/Model.keras\")\n",
        "\n",
        "# Predict the next value after the last sequence\n",
        "last_seq = scaled[-5:].reshape((1, 5, 1))  # last 5 timesteps\n",
        "next_pred_scaled = model.predict(last_seq)\n",
        "\n",
        "# Inverse scale back to original values\n",
        "next_pred = scaler.inverse_transform(next_pred_scaled)\n",
        "print(\"ğŸ”® Next predicted value:\", next_pred[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V4fiqxwNMJs",
        "outputId": "c354e3df-ba87-4c6c-cad5-fc23b5fe812d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step\n",
            "ğŸ”® Next predicted value: 13.919461\n"
          ]
        }
      ]
    }
  ]
}